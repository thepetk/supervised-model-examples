{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BP, BP-F and MLR-F model comparison\n",
    "The notebook aims to compare 3 different approaches. Those are the Multiple Linear Regression (MLR-F), the Neural Network with Back-Propagation and the Neural Network with Back-Propagation (BP-F). For all three approaches we will use the same dataset, which is the one generated by the [generate_preprocessed_dataset.py](./generate_preprocessed_dataset.py) script.\n",
    "\n",
    "## Requirements\n",
    "To run the current notebook you will need to confirm first that you have installed the requirements listed in [requirements.txt](./requirements.txt). Alternatively, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Variables\n",
    "In this section we are going to import all the common libraries and define all static variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from NeuralNet import (\n",
    "    NeuralNet,\n",
    "    relu,\n",
    "    relu_derivative,\n",
    "    sigmoid,\n",
    "    sigmoid_derivative,\n",
    "    tanh,\n",
    "    tanh_derivative,\n",
    "    MOMENTUM,\n",
    "    HIDDEN_LAYERS,\n",
    "    LEARNING_RATE,\n",
    "    NUM_OF_EPOCHS,\n",
    "    OUTPUT_LAYER,\n",
    "    VALIDATION_RATIO,\n",
    ")\n",
    "\n",
    "\n",
    "DATA_TEST_PATH = \"data_test.csv\"\n",
    "DATA_TRAIN_PATH = \"data_train.csv\"\n",
    "PLOT_FIGURE_SIZE = (8, 6)\n",
    "RESULTS_TEST_PATH = \"result_test.csv\"\n",
    "RESULTS_TRAIN_PATH = \"result_train.csv\"\n",
    "\n",
    "X_train = np.genfromtxt(DATA_TRAIN_PATH, delimiter=\",\")\n",
    "y_train = np.genfromtxt(RESULTS_TRAIN_PATH, delimiter=\",\")\n",
    "X_test = np.genfromtxt(DATA_TEST_PATH, delimiter=\",\")\n",
    "y_test = np.genfromtxt(RESULTS_TEST_PATH, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Multiple Linear Regression (MLR-F)\n",
    "For the implementation of this approach we will use `scikit-learn` library. The source code is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "mlr_f = LinearRegression()\n",
    "mlr_f.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "mlr_y_pred = mlr_f.predict(X_test)\n",
    "\n",
    "# Get the MLR F mean square, mean absolute and mean absolute percentage error\n",
    "mlr_mean_square_error = mean_squared_error(y_test, mlr_y_pred)\n",
    "mlr_mean_absolute_error = mean_absolute_error(y_test, mlr_y_pred)\n",
    "mlr_mean_absolute_percentage_error = mean_absolute_percentage_error(y_test, mlr_y_pred)\n",
    "print(f\"MLR-F Mean Square Error:: {mlr_mean_square_error:.6f}\")\n",
    "print(f\"MLR-F Mean Absolute Error:: {mlr_mean_absolute_error:.6f}\")\n",
    "print(f\"MLR-F Mean Absolute Percentage Error:: {mlr_mean_absolute_percentage_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: NeuralNet.py with BackPropagation\n",
    "For the implementation of this approach we will use the neural network already implemented in [NeuralNet.py](./NeuralNet.py). The source code is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of features\n",
    "_, total_features = X_train.shape\n",
    "\n",
    "# Initialize the neural network\n",
    "total_layers = [total_features] + HIDDEN_LAYERS + OUTPUT_LAYER\n",
    "nn_bp = NeuralNet(\n",
    "    total_layers=len(total_layers),\n",
    "    units_per_layer=total_layers,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    momentum=MOMENTUM,\n",
    "    fact=tanh,\n",
    "    fact_derivative=tanh_derivative,\n",
    "    validation_percentage=VALIDATION_RATIO,\n",
    "    num_of_epochs=NUM_OF_EPOCHS,\n",
    ")\n",
    "\n",
    "# Train the neural network\n",
    "nn_bp.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "nn_bp_pred = nn_bp.predict(X_test)\n",
    "\n",
    "# Get the neural network BP mean square, mean absolute and mean absolute percentage error\n",
    "nn_bp_mean_square_error = mean_squared_error(y_test, nn_bp_pred)\n",
    "nn_bp_mean_absolute_error = mean_absolute_error(y_test, nn_bp_pred)\n",
    "nn_bp_mean_absolute_percentage_error = mean_absolute_percentage_error(y_test, nn_bp_pred)\n",
    "\n",
    "print(f\"Neural Network with BP Mean Square Error:: {nn_bp_mean_square_error:.6f}\")\n",
    "print(f\"Neural Network with BP Mean Absolute Error:: {nn_bp_mean_absolute_error:.6f}\")\n",
    "print(f\"Neural Network with BP Mean Absolute Percentage Error:: {nn_bp_mean_absolute_percentage_error:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlr_f.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: Neural Network with BP-F\n",
    "For this implementation we will use `pytorch`. The source code is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size: \"int\", hidden_layers: \"list[int]\", output_size: \"int\"\n",
    "    ) -> None:\n",
    "        super(RegressionModel, self).__init__()\n",
    "\n",
    "        # Initialize the input layer\n",
    "        _layers = [nn.Linear(input_size, hidden_layers[0]), nn.Tanh()]\n",
    "\n",
    "        # Initialize the hidden layers\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            _layers.append(nn.Linear(hidden_layers[i - 1], hidden_layers[i]))\n",
    "            _layers.append(nn.Tanh())\n",
    "\n",
    "        # Initialize the Output layer\n",
    "        _layers.append(nn.Linear(hidden_layers[-1], output_size))\n",
    "\n",
    "        # Combine into a sequential model\n",
    "        self.model = nn.Sequential(*_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Convert X and y to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float().view(-1, 1)\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float().view(-1, 1)\n",
    "\n",
    "# Get the total number of features\n",
    "_, total_features = X_train.shape\n",
    "\n",
    "# Construct the BP-F pytorch neural network model, its loss and its optimizer\n",
    "bp_f = RegressionModel(\n",
    "    input_size=total_features, hidden_layers=HIDDEN_LAYERS, output_size=1\n",
    ")\n",
    "loss_fn = nn.MSELoss(size_average=None, reduce=None)\n",
    "optimizer = optim.Adam(bp_f.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# Training loop (same as NeuralNet\n",
    "for epoch in range(NUM_OF_EPOCHS):\n",
    "    # _forward\n",
    "    predictions = bp_f(X_train_tensor)\n",
    "    loss = loss_fn(predictions, y_train_tensor)\n",
    "    # _backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # _update_weights\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    nn_bp_f_pred = bp_f(X_test_tensor)\n",
    "    test_loss = loss_fn(nn_bp_f_pred, y_test_tensor)\n",
    "    print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "# Get the neural network BP-F mean square, mean absolute and mean absolute percentage error\n",
    "nn_bp_f_mean_square_error = mean_squared_error(y_test, nn_bp_f_pred)\n",
    "nn_bp_f_mean_absolute_error = mean_absolute_error(y_test, nn_bp_f_pred)\n",
    "nn_bp_f_mean_absolute_percentage_error = mean_absolute_percentage_error(y_test, nn_bp_f_pred)\n",
    "\n",
    "print(f\"Neural Network with BP-F Mean Square Error:: {nn_bp_f_mean_square_error:.6f}\")\n",
    "print(f\"Neural Network with BP-F Mean Absolute Error:: {nn_bp_f_mean_absolute_error:.6f}\")\n",
    "print(f\"Neural Network with BP-F Mean Absolute Percentage Error:: {nn_bp_f_mean_absolute_percentage_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations & Plots\n",
    "This section includes visualization (scatter plots) comparing all three models and more specifically we are plotting the predictions with the real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 7))\n",
    "\n",
    "# Scatter plot for Approach 1: Multiple Linear Regression (MLR-F)\n",
    "axes[0].scatter(y_test, mlr_y_pred, color='blue', label='Predictions')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Ideal Prediction Line')\n",
    "axes[0].set_title('Multiple Linear Regression (MLR-F)')\n",
    "axes[0].set_xlabel('Actual Values (zμ)')\n",
    "axes[0].set_ylabel('Predicted Values (yμ)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Scatter plot for Approach 2: NeuralNet.py with BackPropagation\n",
    "axes[1].scatter(y_test, nn_bp_pred, color='green', label='Predictions')\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Ideal Prediction Line')\n",
    "axes[1].set_title('NeuralNet.py with BP')\n",
    "axes[1].set_xlabel('Actual Values (zμ)')\n",
    "axes[1].set_ylabel('Predicted Values (yμ)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Scatter plot for Approach 3: NeuralNet.py with BackPropagation\n",
    "axes[2].scatter(y_test, nn_bp_f_pred, color='red', label='Predictions')\n",
    "axes[2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Ideal Prediction Line')\n",
    "axes[2].set_title('PyTorch NN with BP')\n",
    "axes[2].set_xlabel('Actual Values (zμ)')\n",
    "axes[2].set_ylabel('Predicted Values (yμ)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
